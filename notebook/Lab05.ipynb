{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0z1hpobEXXAR",
        "outputId": "b52a3664-9436-4e5a-d9a5-2dd578741be2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lab 5 — PyTorch: Summary results\n",
            "Part 1: tensor matmul:\n",
            " tensor([[ 5, 11],\n",
            "        [11, 25]])\n",
            "Part 1: reshaped shape: torch.Size([16, 1])\n",
            "Part 2: grad after first backward: 18.0\n",
            "Part 2: grad after recompute+backward: 18.0\n",
            "Part 2: grad after retain_graph once: 18.0\n",
            "Part 2: grad after retain_graph twice (accumulated): 36.0\n",
            "Part 3: linear_out_shape: torch.Size([3, 2])\n",
            "Part 3: emb_out_shape: torch.Size([4, 3])\n",
            "Part 3: model_out_shape: torch.Size([1, 2])\n",
            "Training demo losses (per batch): [0.6951454877853394, 0.707666277885437, 0.6541890501976013, 0.6947219371795654, 0.6914669275283813]\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Lab 5 — Introduction to PyTorch\n",
        "\"\"\"\n",
        "\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch import nn\n",
        "\n",
        "# Reproducibility\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)\n",
        "\n",
        "def part1_tensors():\n",
        "    \"\"\"Part 1: create tensors, demonstrate operations, indexing and reshape.\"\"\"\n",
        "    data = [[1, 2], [3, 4]]\n",
        "    x_data = torch.tensor(data)\n",
        "    x_np = torch.from_numpy(np.array(data))\n",
        "    ones = torch.ones_like(x_data)\n",
        "    rand = torch.rand_like(x_data, dtype=torch.float)\n",
        "    matmul = x_data @ x_data.T\n",
        "    arr = torch.rand(4, 4)\n",
        "    reshaped = arr.view(16, 1)\n",
        "    return {\n",
        "        \"x_data\": x_data,\n",
        "        \"x_np\": x_np,\n",
        "        \"ones\": ones,\n",
        "        \"rand\": rand,\n",
        "        \"matmul\": matmul,\n",
        "        \"reshaped_shape\": reshaped.shape\n",
        "    }\n",
        "\n",
        "def part2_autograd():\n",
        "    \"\"\"Part 2: autograd demonstration and safe patterns for repeated backward calls.\"\"\"\n",
        "    x = torch.ones(1, requires_grad=True)\n",
        "    # z = 3 * (x + 2)^2\n",
        "    z = 3 * ((x + 2) ** 2)\n",
        "    # Standard use: compute backward once and inspect gradient\n",
        "    if x.grad is not None:\n",
        "        x.grad.zero_()\n",
        "    z.backward()\n",
        "    grad_after_first = x.grad.clone()\n",
        "\n",
        "    # Safe pattern: zero grad and recompute forward before backward again\n",
        "    x.grad.zero_()\n",
        "    z = 3 * ((x + 2) ** 2)  # recompute for fresh graph\n",
        "    z.backward()\n",
        "    grad_after_recompute = x.grad.clone()\n",
        "\n",
        "    # Debug pattern only: retain_graph=True to allow multiple backward calls on same graph\n",
        "    x.grad.zero_()\n",
        "    z = 3 * ((x + 2) ** 2)\n",
        "    z.backward(retain_graph=True)\n",
        "    grad_after_retain_once = x.grad.clone()\n",
        "    z.backward(retain_graph=True)\n",
        "    grad_after_retain_twice = x.grad.clone()\n",
        "\n",
        "    # Clear gradient for cleanliness\n",
        "    x.grad.zero_()\n",
        "\n",
        "    return {\n",
        "        \"grad_after_first\": grad_after_first,\n",
        "        \"grad_after_recompute\": grad_after_recompute,\n",
        "        \"grad_after_retain_once\": grad_after_retain_once,\n",
        "        \"grad_after_retain_twice\": grad_after_retain_twice\n",
        "    }\n",
        "\n",
        "class SimpleModel(nn.Module):\n",
        "    \"\"\"Simple model with embedding + mean pooling + two linear layers (small example).\"\"\"\n",
        "    def __init__(self, vocab_size: int, emb_dim: int, hid_dim: int, out_dim: int):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_dim)\n",
        "        self.lin = nn.Linear(emb_dim, hid_dim)\n",
        "        self.act = nn.ReLU()\n",
        "        self.out = nn.Linear(hid_dim, out_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Expect x shape: (batch_size, seq_len)\n",
        "        e = self.embedding(x)  # (batch, seq_len, emb_dim)\n",
        "        if e.dim() == 3:\n",
        "            v = e.mean(dim=1)   # mean pooling across sequence length\n",
        "        else:\n",
        "            v = e.mean(dim=0)\n",
        "        h = self.act(self.lin(v))\n",
        "        return self.out(h)\n",
        "\n",
        "def part3_nn_examples():\n",
        "    \"\"\"Part 3: demonstrate Linear, Embedding, and SimpleModel.\"\"\"\n",
        "    linear = nn.Linear(5, 2)\n",
        "    sample_in = torch.randn(3, 5)\n",
        "    linear_out = linear(sample_in)\n",
        "\n",
        "    embedding = nn.Embedding(10, 3)\n",
        "    emb_indices = torch.LongTensor([1, 5, 0, 8])\n",
        "    emb_out = embedding(emb_indices)\n",
        "\n",
        "    model = SimpleModel(vocab_size=100, emb_dim=16, hid_dim=8, out_dim=2)\n",
        "    model_in = torch.LongTensor([[1, 2, 5, 9]])\n",
        "    model_out = model(model_in)\n",
        "\n",
        "    return {\n",
        "        \"linear_out_shape\": linear_out.shape,\n",
        "        \"emb_out_shape\": emb_out.shape,\n",
        "        \"model_out_shape\": model_out.shape,\n",
        "        \"model_out\": model_out.detach()\n",
        "    }\n",
        "\n",
        "def training_demo_synthetic(num_samples=20, seq_len=6, vocab_size=50):\n",
        "    \"\"\"Small training loop demo on synthetic data (1 epoch).\"\"\"\n",
        "    X = torch.randint(0, vocab_size, (num_samples, seq_len), dtype=torch.long)\n",
        "    Y = torch.randint(0, 2, (num_samples,), dtype=torch.long)\n",
        "    model = SimpleModel(vocab_size=vocab_size, emb_dim=16, hid_dim=8, out_dim=2)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
        "    batch_size = 4\n",
        "    losses = []\n",
        "    model.train()\n",
        "    for i in range(0, num_samples, batch_size):\n",
        "        xb = X[i:i+batch_size]\n",
        "        yb = Y[i:i+batch_size]\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(xb)\n",
        "        loss = criterion(logits, yb)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        losses.append(loss.item())\n",
        "    return losses\n",
        "\n",
        "def main():\n",
        "    # Execute parts and print concise results for instructor\n",
        "    p1 = part1_tensors()\n",
        "    p2 = part2_autograd()\n",
        "    p3 = part3_nn_examples()\n",
        "    train_losses = training_demo_synthetic()\n",
        "\n",
        "    # Minimal, formal output\n",
        "    print(\"Lab 5 — PyTorch: Summary results\")\n",
        "    print(\"Part 1: tensor matmul:\\n\", p1[\"matmul\"])\n",
        "    print(\"Part 1: reshaped shape:\", p1[\"reshaped_shape\"])\n",
        "    print(\"Part 2: grad after first backward:\", p2[\"grad_after_first\"].item())\n",
        "    print(\"Part 2: grad after recompute+backward:\", p2[\"grad_after_recompute\"].item())\n",
        "    print(\"Part 2: grad after retain_graph once:\", p2[\"grad_after_retain_once\"].item())\n",
        "    print(\"Part 2: grad after retain_graph twice (accumulated):\", p2[\"grad_after_retain_twice\"].item())\n",
        "    print(\"Part 3: linear_out_shape:\", p3[\"linear_out_shape\"])\n",
        "    print(\"Part 3: emb_out_shape:\", p3[\"emb_out_shape\"])\n",
        "    print(\"Part 3: model_out_shape:\", p3[\"model_out_shape\"])\n",
        "    print(\"Training demo losses (per batch):\", train_losses)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ]
    }
  ]
}
