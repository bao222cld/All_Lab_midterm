{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aqfM4hbIF3QU",
        "outputId": "aef90a33-38bb-4607-baec-9b8f28523fb0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Collecting seqeval\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.36.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.12/dist-packages (from seqeval) (1.6.1)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2025.11.12)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.6.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16162 sha256=5bd0523e58428d6874c7b42122b6157993c665ef4b5cca49ed3aa2fb440b5837\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/b8/73/0b2c1a76b701a677653dd79ece07cfabd7457989dbfbdcd8d7\n",
            "Successfully built seqeval\n",
            "Installing collected packages: seqeval\n",
            "Successfully installed seqeval-1.2.2\n"
          ]
        }
      ],
      "source": [
        "# Cell 1: Cài đặt Thư viện Cần Thiết\n",
        "!pip install datasets torch numpy seqeval"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Tải và Tiền xử lý Dữ liệu (SỬA LỖI ATTRIBUTEERROR)\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from collections import Counter\n",
        "\n",
        "# 1. Tải dữ liệu CoNLL 2003 từ phiên bản mirror đã được lưu trữ (ổn định)\n",
        "print(\"Đang tải bộ dữ liệu CoNLL 2003 (phiên bản ổn định)...\")\n",
        "dataset = load_dataset(\"lhoestq/conll2003\")\n",
        "print(\"Tải dữ liệu hoàn tất.\")\n",
        "print(dataset)\n",
        "\n",
        "# 2. Trích xuất câu và nhãn\n",
        "train_sentences = dataset[\"train\"][\"tokens\"]\n",
        "train_tags_numerical = dataset[\"train\"][\"ner_tags\"]\n",
        "val_sentences = dataset[\"validation\"][\"tokens\"]\n",
        "val_tags_numerical = dataset[\"validation\"][\"ner_tags\"]\n",
        "\n",
        "# Sửa lỗi AttributeError: Định nghĩa tên nhãn CoNLL 2003 tiêu chuẩn (9 classes)\n",
        "tag_names = [\n",
        "    'O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC'\n",
        "]\n",
        "print(f\"\\nCác nhãn NER (9 classes): {tag_names}\")\n",
        "\n",
        "# Chuyển đổi nhãn số sang nhãn string (sử dụng tag_names đã định nghĩa)\n",
        "def convert_numerical_tags_to_string(numerical_tags_list):\n",
        "    string_tags_list = []\n",
        "    for tags in numerical_tags_list:\n",
        "        string_tags_list.append([tag_names[tag] for tag in tags])\n",
        "    return string_tags_list\n",
        "\n",
        "train_tags = convert_numerical_tags_to_string(train_tags_numerical)\n",
        "val_tags = convert_numerical_tags_to_string(val_tags_numerical)\n",
        "\n",
        "# 3. Xây dựng Từ điển (Vocabulary)\n",
        "word_counts = Counter(word for sentence in train_sentences for word in sentence)\n",
        "unique_words = sorted(word_counts.keys())\n",
        "\n",
        "# Ánh xạ từ -> index\n",
        "word_to_ix = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
        "for word in unique_words:\n",
        "    if word not in word_to_ix:\n",
        "        word_to_ix[word] = len(word_to_ix)\n",
        "\n",
        "# Ánh xạ nhãn -> index (dùng tag_names đã định nghĩa)\n",
        "tag_to_ix = {}\n",
        "for tag in tag_names:\n",
        "    if tag not in tag_to_ix:\n",
        "        tag_to_ix[tag] = len(tag_to_ix)\n",
        "\n",
        "ix_to_tag = {v: k for k, v in tag_to_ix.items()}\n",
        "\n",
        "# In ra kích thước từ điển\n",
        "VOCAB_SIZE = len(word_to_ix)\n",
        "OUTPUT_SIZE = len(tag_to_ix)\n",
        "print(\"\\n--- Kích thước Từ điển ---\")\n",
        "print(f\"Kích thước từ điển từ (VOCAB_SIZE): {VOCAB_SIZE}\")\n",
        "print(f\"Kích thước từ điển nhãn (OUTPUT_SIZE): {OUTPUT_SIZE}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hv6yezCXF8rA",
        "outputId": "41667799-386a-41a5-b4d6-e43cdb01aff2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Đang tải bộ dữ liệu CoNLL 2003 (phiên bản ổn định)...\n",
            "Tải dữ liệu hoàn tất.\n",
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
            "        num_rows: 14041\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
            "        num_rows: 3250\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
            "        num_rows: 3453\n",
            "    })\n",
            "})\n",
            "\n",
            "Các nhãn NER (9 classes): ['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']\n",
            "\n",
            "--- Kích thước Từ điển ---\n",
            "Kích thước từ điển từ (VOCAB_SIZE): 23625\n",
            "Kích thước từ điển nhãn (OUTPUT_SIZE): 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Task 2 - Tạo PyTorch Dataset và DataLoader\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import numpy as np\n",
        "\n",
        "# Giá trị đặc biệt để đệm nhãn. Phải là giá trị không được dùng trong tag_to_ix\n",
        "PADDING_TAG_INDEX = -1\n",
        "\n",
        "# 1. Tạo lớp NERDataset\n",
        "class NERDataset(Dataset):\n",
        "    def __init__(self, sentences, tags, word_to_ix, tag_to_ix):\n",
        "        self.sentences = sentences\n",
        "        self.tags = tags\n",
        "        self.word_to_ix = word_to_ix\n",
        "        self.tag_to_ix = tag_to_ix\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sentence = self.sentences[idx]\n",
        "        tags = self.tags[idx]\n",
        "\n",
        "        # Chuyển đổi từ/nhãn sang chỉ số, dùng <UNK> cho từ không biết\n",
        "        sentence_indices = [self.word_to_ix.get(word, self.word_to_ix[\"<UNK>\"]) for word in sentence]\n",
        "        # Chuyển đổi nhãn sang chỉ số\n",
        "        tag_indices = [self.tag_to_ix[tag] for tag in tags]\n",
        "\n",
        "        return (torch.tensor(sentence_indices, dtype=torch.long),\n",
        "                torch.tensor(tag_indices, dtype=torch.long))\n",
        "\n",
        "# 2. Tạo DataLoader và Hàm collate_fn\n",
        "def collate_fn(batch):\n",
        "    sentences = [item[0] for item in batch]\n",
        "    tags = [item[1] for item in batch]\n",
        "\n",
        "    # Đệm (padding) các câu. Sử dụng index của <PAD> (0)\n",
        "    sentences_padded = pad_sequence(sentences, batch_first=True, padding_value=word_to_ix[\"<PAD>\"])\n",
        "\n",
        "    # Đệm (padding) các nhãn. Sử dụng giá trị đặc biệt -1\n",
        "    tags_padded = pad_sequence(tags, batch_first=True, padding_value=PADDING_TAG_INDEX)\n",
        "\n",
        "    return sentences_padded, tags_padded\n",
        "\n",
        "# Khởi tạo Datasets\n",
        "# Biến train_sentences, train_tags, val_sentences, val_tags được khởi tạo ở Cell 2\n",
        "train_dataset = NERDataset(train_sentences, train_tags, word_to_ix, tag_to_ix)\n",
        "val_dataset = NERDataset(val_sentences, val_tags, word_to_ix, tag_to_ix)\n",
        "\n",
        "# Khởi tạo DataLoaders\n",
        "BATCH_SIZE = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "print(f\"Số lượng mẫu huấn luyện: {len(train_dataset)}\")\n",
        "print(f\"Số lượng batch trong Train DataLoader: {len(train_loader)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O0Z0ffN_GBpg",
        "outputId": "d81155c6-423a-472b-c52c-a53e192dd135"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Số lượng mẫu huấn luyện: 14041\n",
            "Số lượng batch trong Train DataLoader: 439\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Task 3 - Xây dựng Mô hình RNN (Bi-LSTM)\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Định nghĩa kiến trúc Bi-LSTM\n",
        "class SimpleRNNForTokenClassification(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_size):\n",
        "        super(SimpleRNNForTokenClassification, self).__init__()\n",
        "\n",
        "        # 1. Lớp Embedding\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        # 2. Lớp LSTM (Bi-directional)\n",
        "        # Bidirectional=True sẽ tạo ra output có kích thước 2 * hidden_dim\n",
        "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
        "\n",
        "        # 3. Lớp Linear để ánh xạ từ hidden state sang không gian nhãn\n",
        "        self.linear = nn.Linear(hidden_dim * 2, output_size)\n",
        "\n",
        "    def forward(self, sentences):\n",
        "        embedded = self.embedding(sentences)\n",
        "        # rnn_output: (batch_size, seq_len, 2 * hidden_dim)\n",
        "        rnn_output, _ = self.rnn(embedded)\n",
        "        # scores: (batch_size, seq_len, output_size)\n",
        "        scores = self.linear(rnn_output)\n",
        "\n",
        "        return scores\n",
        "\n",
        "# Khởi tạo tham số mô hình\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 256\n",
        "\n",
        "# Khởi tạo mô hình và chuyển sang GPU/CPU\n",
        "# Các biến VOCAB_SIZE, OUTPUT_SIZE được khởi tạo ở Cell 2\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = SimpleRNNForTokenClassification(VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_SIZE).to(device)\n",
        "\n",
        "print(model)\n",
        "print(f\"Mô hình được chuyển sang thiết bị: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kO_aP0aEGfYA",
        "outputId": "36a5ee12-17c2-4a0e-bc35-b34f82a27d4d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SimpleRNNForTokenClassification(\n",
            "  (embedding): Embedding(23625, 100)\n",
            "  (rnn): LSTM(100, 256, batch_first=True, bidirectional=True)\n",
            "  (linear): Linear(in_features=512, out_features=9, bias=True)\n",
            ")\n",
            "Mô hình được chuyển sang thiết bị: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Task 4 - Huấn luyện Mô hình\n",
        "LEARNING_RATE = 0.005\n",
        "NUM_EPOCHS = 5\n",
        "\n",
        "# Optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# Loss function: nn.CrossEntropyLoss\n",
        "# ignore_index=PADDING_TAG_INDEX (-1): bỏ qua loss của các token đệm\n",
        "# PADDING_TAG_INDEX được định nghĩa ở Cell 3\n",
        "loss_function = nn.CrossEntropyLoss(ignore_index=PADDING_TAG_INDEX)\n",
        "\n",
        "print(\"\\n--- Bắt đầu Huấn luyện Mô hình ---\")\n",
        "\n",
        "for epoch in range(1, NUM_EPOCHS + 1):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for sentences, tags in train_loader:\n",
        "        sentences = sentences.to(device)\n",
        "        tags = tags.to(device)\n",
        "\n",
        "        # (1) Xóa gradient cũ\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # (2) Forward pass\n",
        "        scores = model(sentences)\n",
        "\n",
        "        # Reshape cho CrossEntropyLoss: (Batch * Seq_len, Output_size) và (Batch * Seq_len)\n",
        "        scores = scores.view(-1, scores.shape[-1])\n",
        "        tags = tags.view(-1)\n",
        "\n",
        "        # (3) Tính loss\n",
        "        loss = loss_function(scores, tags)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # (4) Backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # (5) Cập nhật trọng số\n",
        "        optimizer.step()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch}/{NUM_EPOCHS}, Loss trung bình: {avg_loss:.4f}\")\n",
        "\n",
        "print(\"--- Huấn luyện hoàn tất ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65dwdwZLGkV4",
        "outputId": "b1ade444-d2ee-4add-9e5c-31b30b619666"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Bắt đầu Huấn luyện Mô hình ---\n",
            "Epoch 1/5, Loss trung bình: 0.3331\n",
            "Epoch 2/5, Loss trung bình: 0.0788\n",
            "Epoch 3/5, Loss trung bình: 0.0182\n",
            "Epoch 4/5, Loss trung bình: 0.0052\n",
            "Epoch 5/5, Loss trung bình: 0.0017\n",
            "--- Huấn luyện hoàn tất ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Task 5 - Đánh giá Mô hình và Dự đoán\n",
        "from seqeval.metrics import classification_report\n",
        "\n",
        "# 1. Viết hàm evaluate\n",
        "def evaluate(model, data_loader, device):\n",
        "    model.eval()\n",
        "    total_correct = 0\n",
        "    total_non_padding_tokens = 0\n",
        "    all_predictions = []\n",
        "    all_true_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for sentences, tags in data_loader:\n",
        "            sentences = sentences.to(device)\n",
        "            tags = tags.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            scores = model(sentences)\n",
        "            # Lấy dự đoán\n",
        "            predictions = torch.argmax(scores, dim=-1)\n",
        "\n",
        "            # Lọc các vị trí padding\n",
        "            # PADDING_TAG_INDEX được định nghĩa ở Cell 3\n",
        "            mask = (tags != PADDING_TAG_INDEX)\n",
        "\n",
        "            # Tính toán độ chính xác (Token Accuracy)\n",
        "            correct = (predictions == tags) & mask\n",
        "            total_correct += correct.sum().item()\n",
        "            total_non_padding_tokens += mask.sum().item()\n",
        "\n",
        "            # Chuẩn bị cho seqeval\n",
        "            for i in range(sentences.shape[0]):\n",
        "                # Lấy độ dài thực của câu (không tính padding)\n",
        "                sentence_len = (tags[i] != PADDING_TAG_INDEX).sum().item()\n",
        "\n",
        "                true_sentence_tags = tags[i, :sentence_len].cpu().numpy()\n",
        "                pred_sentence_tags = predictions[i, :sentence_len].cpu().numpy()\n",
        "\n",
        "                # Chuyển index thành tên nhãn string (sử dụng ix_to_tag từ Cell 2)\n",
        "                all_true_labels.append([ix_to_tag[t] for t in true_sentence_tags])\n",
        "                all_predictions.append([ix_to_tag[p] for p in pred_sentence_tags])\n",
        "\n",
        "    # 2. Báo cáo kết quả\n",
        "    token_accuracy = total_correct / total_non_padding_tokens if total_non_padding_tokens > 0 else 0\n",
        "    print(f\"\\nĐộ chính xác trên tập validation (Token Accuracy): {token_accuracy:.4f}\")\n",
        "\n",
        "    print(\"\\n--- Báo cáo NER chi tiết (seqeval) ---\")\n",
        "    # seqeval tính Precision, Recall, F1-score cho từng loại thực thể\n",
        "    print(classification_report(all_true_labels, all_predictions, zero_division=0))\n",
        "\n",
        "    return token_accuracy\n",
        "\n",
        "# Đánh giá mô hình trên tập validation\n",
        "val_accuracy = evaluate(model, val_loader, device)\n",
        "\n",
        "\n",
        "# 3. Viết hàm predict_sentence\n",
        "def predict_sentence(sentence_str, model, word_to_ix, ix_to_tag, device):\n",
        "    model.eval()\n",
        "\n",
        "    tokens = sentence_str.split()\n",
        "    # Chuyển đổi từ thành index, dùng <UNK> nếu từ không có trong từ điển (sử dụng word_to_ix từ Cell 2)\n",
        "    token_indices = [word_to_ix.get(word, word_to_ix[\"<UNK>\"]) for word in tokens]\n",
        "\n",
        "    input_tensor = torch.tensor([token_indices], dtype=torch.long).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        scores = model(input_tensor)\n",
        "        # Lấy nhãn có điểm số cao nhất\n",
        "        predictions = torch.argmax(scores, dim=-1).squeeze(0).cpu().numpy()\n",
        "\n",
        "    predicted_tags = [ix_to_tag[p] for p in predictions]\n",
        "\n",
        "    # In ra các cặp (từ, nhãn dự đoán)\n",
        "    print(\"\\n--- Ví dụ dự đoán câu mới ---\")\n",
        "    print(f\"Câu: {sentence_str}\")\n",
        "    print(\"Dự đoán:\")\n",
        "    for word, tag in zip(tokens, predicted_tags):\n",
        "        print(f\"({word}, {tag})\")\n",
        "\n",
        "# Ví dụ dự đoán câu mới (các thực thể CoNLL 2003: PER, ORG, LOC, MISC)\n",
        "new_sentence = \"The New York Times reported that Joe Biden visited London yesterday\"\n",
        "predict_sentence(new_sentence, model, word_to_ix, ix_to_tag, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C4V_2Gu_Gmnn",
        "outputId": "7538eb9d-aad0-40d5-fe7f-cb7d1042b9ae"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Độ chính xác trên tập validation (Token Accuracy): 0.9547\n",
            "\n",
            "--- Báo cáo NER chi tiết (seqeval) ---\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         LOC       0.93      0.81      0.86      1837\n",
            "        MISC       0.89      0.77      0.82       922\n",
            "         ORG       0.51      0.81      0.63      1341\n",
            "         PER       0.77      0.70      0.74      1842\n",
            "\n",
            "   micro avg       0.74      0.77      0.75      5942\n",
            "   macro avg       0.77      0.77      0.76      5942\n",
            "weighted avg       0.78      0.77      0.76      5942\n",
            "\n",
            "\n",
            "--- Ví dụ dự đoán câu mới ---\n",
            "Câu: The New York Times reported that Joe Biden visited London yesterday\n",
            "Dự đoán:\n",
            "(The, B-ORG)\n",
            "(New, I-ORG)\n",
            "(York, I-ORG)\n",
            "(Times, I-ORG)\n",
            "(reported, O)\n",
            "(that, O)\n",
            "(Joe, B-PER)\n",
            "(Biden, O)\n",
            "(visited, O)\n",
            "(London, B-LOC)\n",
            "(yesterday, O)\n"
          ]
        }
      ]
    }
  ]
}