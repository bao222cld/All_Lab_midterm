# -*- coding: utf-8 -*-
"""NLP_Lab3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HlvPwhX8zloOM1hIjmiIO8Ww9gxdGN0D
"""

# Import các thư viện cần thiết
from pyspark.sql import SparkSession
from pyspark.ml import Pipeline
from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, HashingTF, IDF
from datetime import datetime
import json, os, glob, shutil, traceback

# 1. Khởi tạo SparkSession (cần để làm việc với Spark)
# .master("local[*]") : chạy local với tất cả CPU cores
# .appName("...")     : đặt tên cho ứng dụng Spark
spark = SparkSession.builder \
    .master("local[*]") \
    .appName("Lab17 PySpark Colab") \
    .getOrCreate()
print("Spark version:", spark.version)

# 2. Đường dẫn tới file dữ liệu C4 (đặt trong Google Drive)
input_path = "/content/drive/MyDrive/c4-train.00000-of-01024-30K.json.gz"

# Kiểm tra file có tồn tại không, nếu không báo lỗi
if not os.path.exists(input_path):
    raise FileNotFoundError(f"Không tìm thấy file: {input_path}. Upload file hoặc chỉnh input_path đúng chỗ.")

# 3. Đọc dữ liệu JSON vào DataFrame Spark
df = spark.read.json(input_path)
print("Schema của DataFrame:")
df.printSchema()

# Hiển thị 3 dòng mẫu để kiểm tra nội dung
print("Sample (3 dòng):")
display(df.limit(3).toPandas())

# 4. Giới hạn chỉ lấy 1000 dòng để xử lý (chạy nhanh hơn, tránh tốn bộ nhớ)
df_small = df.limit(1000)
print("Số dòng sample dùng:", df_small.count())

# 5. Xác định tên cột chứa văn bản
text_col_name = "text"

# Nếu không có cột "text" thì báo lỗi
if text_col_name not in df_small.columns:
    raise ValueError(f"Không thấy cột '{text_col_name}' trong DataFrame. Các cột hiện có: {df_small.columns}")

# 6. Tạo các bước xử lý (pipeline stages)

# (a) RegexTokenizer: tách văn bản thành tokens dựa theo regex (\\W+ = ký tự không phải chữ)
regexTokenizer = RegexTokenizer(inputCol=text_col_name, outputCol="tokens_regex", pattern="\\W+")

# (b) StopWordsRemover: loại bỏ các từ dừng (stop words)
stopwordsRemover = StopWordsRemover(inputCol="tokens_regex", outputCol="tokens_clean")

# (c) HashingTF: chuyển tokens thành vector tần suất (TF)
hashingTF = HashingTF(inputCol="tokens_clean", outputCol="rawFeatures", numFeatures=20000)

# (d) IDF: tính trọng số TF-IDF
idf = IDF(inputCol="rawFeatures", outputCol="features")

# Gộp các bước thành Pipeline
pipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover, hashingTF, idf])

# 7. Chạy pipeline và log quá trình
start = datetime.utcnow().isoformat() + "Z"
try:
    # Huấn luyện pipeline (fit) và biến đổi dữ liệu (transform)
    model = pipeline.fit(df_small)
    result = model.transform(df_small)

    # Hiển thị 3 dòng kết quả (cột text và tokens_clean sau xử lý)
    result.select(text_col_name, "tokens_clean").show(3, truncate=120)

    # --- LƯU KẾT QUẢ ---
    # 1. Lưu dạng Parquet (tiện cho việc đọc lại trong Spark/Scala)
    out_parquet = "/content/drive/MyDrive/lab17/output_parquet"
    try:
        if os.path.exists(out_parquet):
            shutil.rmtree(out_parquet)  # xóa nếu đã tồn tại
    except Exception:
        pass
    result.select(text_col_name, "tokens_clean", "features").write.mode("overwrite").parquet(out_parquet)
    print("Đã lưu parquet vào:", out_parquet)

    # 2. Lưu dạng file text (JSON Lines) với thông tin rút gọn
    tmp_dir = "/content/lab17_output_tmp"
    if os.path.exists(tmp_dir):
        shutil.rmtree(tmp_dir)

    # Mỗi dòng gồm: text (tối đa 500 ký tự), tokens (tối đa 100 từ), độ dài vector features
    rdd = result.rdd.map(lambda r: json.dumps({
        "text": (r[text_col_name][:500] if r[text_col_name] else ""),
        "tokens": (r["tokens_clean"][:100] if r["tokens_clean"] else []),
        "features_len": int(r["features"].size) if r["features"] else 0
    }, ensure_ascii=False))

    # Gộp về 1 file (coalesce(1)) để dễ nộp
    rdd.coalesce(1).saveAsTextFile(tmp_dir)

    # Copy part-* ra file txt duy nhất
    parts = glob.glob(tmp_dir + "/part-*")
    if not parts:
        raise RuntimeError("Không tìm thấy part-* khi lưu text output.")
    final_local = "/content/lab17_pipeline_output.txt"
    shutil.copy(parts[0], final_local)

    # Copy vào Google Drive để lưu lại
    final_drive = "/content/drive/MyDrive/lab17/lab17_pipeline_output.txt"
    os.makedirs("/content/drive/MyDrive/lab17", exist_ok=True)
    shutil.copy(final_local, final_drive)

    # --- GHI LOG ---
    end = datetime.utcnow().isoformat() + "Z"
    log_text = f"Start: {start}\nEnd: {end}\nStatus: SUCCESS\nRows_used: {df_small.count()}\n"
    log_drive = "/content/drive/MyDrive/lab17/lab17_run_log.txt"
    with open(log_drive, "w", encoding="utf-8") as f:
        f.write(log_text)

    print("Kết thúc thành công.")
    print("Text output (local):", final_local)
    print("Text output (Drive):", final_drive)
    print("Log (Drive):", log_drive)

# 8. Nếu có lỗi thì ghi log lỗi vào file
except Exception as e:
    err = traceback.format_exc()
    log_err = f"Start: {start}\nEnd: {datetime.utcnow().isoformat()}Z\nStatus: ERROR\n{err}\n"
    os.makedirs("/content/drive/MyDrive/lab17", exist_ok=True)
    with open("/content/drive/MyDrive/lab17/lab17_run_log.txt", "w", encoding="utf-8") as f:
        f.write(log_err)
    print("Có lỗi — log đã lưu vào /content/drive/MyDrive/lab17/lab17_run_log.txt")
    raise